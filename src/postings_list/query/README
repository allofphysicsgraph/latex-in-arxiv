12MB/s of source docs with print_tokens
tokenize, basic counting,tf-idf, sync results to two files offsets and tf_idf

~25MB/s with print_tokens turned off.

XXH3_64bits runs at 60GB/s if AVX2 is enabled.
Is it feasible to have tokenizers performe similarly?


MAX_FILES 100000
MAX_TOKEN_LEN 2000
can be updated in globals.h


2003 61MB
./scanner.out .  2.23s user 0.82s system 99% cpu 3.058 total
id:84b99657759c41b3: count:155 docs:2 tf_idf:8.642887 tok:\end{fmfgraph*}
id:867fd8d232acad40: count:165 docs:2 tf_idf:8.715923 tok:{v5}
id:cb569c94412513a: count:79 docs:1 tf_idf:8.733001 tok:\fmfforce{1w,0h}
id:22b55d7ebdcd6d2d: count:79 docs:1 tf_idf:8.733001 tok:{v1,v2,v1}
id:55a3dc6817022949: count:322 docs:3 tf_idf:8.878983 tok:{v1}
id:6d9daf65ee149dd0: count:743 docs:5 tf_idf:8.933640 tok:{rgb}
id:6f319cf9c584f8b8: count:94 docs:1 tf_idf:8.957511 tok:\stackrel{ab}
id:a47b3c3129a0149a: count:101 docs:1 tf_idf:9.050393 tok:{v6}
id:ab7ced07c3097022: count:105 docs:1 tf_idf:9.100647 tok:{f^{*}}
id:dfc673f5734ce0c5: count:108 docs:1 tf_idf:9.137108 tok:{d^{*}}
id:29646a6489295fc4: count:253 docs:2 tf_idf:9.215924 tok:{v4}
id:e13a454bbfc5056: count:116 docs:1 tf_idf:9.229637 tok:( x, \lambda )
id:1c141a06a0b50491: count:277 docs:2 tf_idf:9.322057 tok:{v3}
id:57870eb89cdfe230: count:131 docs:1 tf_idf:9.387230 tok:{{a_{1}}}
id:7fbe171b68d94d5: count:135 docs:1 tf_idf:9.426231 tok:{{a_{1}^{*}}}
id:2892be74ab087182: count:324 docs:2 tf_idf:9.505676 tok:{v2}
id:e154bcff75b97b9e: count:177 docs:1 tf_idf:9.777830 tok:\begin{eq}
id:391f0e8d723ebbd5: count:177 docs:1 tf_idf:9.777830 tok:\end{eq}
id:79c4b17f8f89019: count:186 docs:1 tf_idf:9.842269 tok:$ & $
id:865f0a26b84cf190: count:189 docs:1 tf_idf:9.863062 tok:\fmf{plain,left=1}
id:4667e62a8eda2a98: count:210 docs:1 tf_idf:10.000021 tok:%%@


25 bytes of context to the left
50 to the right
context window can be set in reader.rl

./scanner.out . offsets 
47357<<47332:47417:86>>

