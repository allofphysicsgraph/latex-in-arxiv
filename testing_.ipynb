{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c729127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://arxiv.org/e-print/2004.04818\n",
    "#!wget https://mirrors.ctan.org/obsolete/macros/latex/contrib/revtex4-1.zip\n",
    "#!unzip revtex4-1.zip\n",
    "#!cd revtex4-1/\n",
    "#run in terminal\n",
    "#find . -maxdepth 1 -type f -name \"*.dtx\" -exec  tex \"{}\" \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6b303024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sys import argv\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import mwe\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#from pudb import set_trace\n",
    "argv=[]\n",
    "argv.append('')\n",
    "argv.append('sound1.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8ad5cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_display_latex(tex):\n",
    "    import re\n",
    "    resp = re.sub(r'{\\\\rm ','{',tex)\n",
    "    resp = re.sub(r'\\\\left\\(','(',resp)\n",
    "    resp = re.sub(r'\\\\right\\)',')',resp)\n",
    "    resp = re.sub(r'{\\\\it ','{',resp)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60796e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trie(dict):\n",
    "    #https://github.com/nltk/nltk/blob/develop/nltk/collections.py\n",
    "    \"\"\"A Trie implementation for strings\"\"\"\n",
    "\n",
    "    LEAF = True\n",
    "\n",
    "    def __init__(self, strings=None):\n",
    "        \"\"\"Builds a Trie object, which is built around a ``dict``\n",
    "\n",
    "        If ``strings`` is provided, it will add the ``strings``, which\n",
    "        consist of a ``list`` of ``strings``, to the Trie.\n",
    "        Otherwise, it'll construct an empty Trie.\n",
    "\n",
    "        :param strings: List of strings to insert into the trie\n",
    "            (Default is ``None``)\n",
    "        :type strings: list(str)\n",
    "\n",
    "        \"\"\"\n",
    "        super(Trie, self).__init__()\n",
    "        if strings:\n",
    "            for string in strings:\n",
    "                self.insert(string)\n",
    "\n",
    "    def insert(self, string):\n",
    "        if len(string):\n",
    "            self[string[0]].insert(string[1:])\n",
    "        else:\n",
    "            if not self[Trie.LEAF]:\n",
    "                self[Trie.LEAF] = 1\n",
    "            else:\n",
    "                self[Trie.LEAF]+=1\n",
    "\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        self[key] = Trie()\n",
    "        return self[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b309d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_token(string):\n",
    "    tokenizer.add_mwe(r\"{}\".format(string))\n",
    "\n",
    "\n",
    "def manual_entries(pattern, data):\n",
    "    resp = re.findall(pattern, data)\n",
    "    if resp:\n",
    "        for match in resp:\n",
    "            add_new_token(match)\n",
    "\n",
    "\n",
    "def print_symbols_not_used():\n",
    "    for k, v in symbols_use_count.items():\n",
    "        if v == 0:\n",
    "            print(k)\n",
    "\n",
    "\n",
    "def read_file(path, f_name):\n",
    "    with open(\"{}/{}\".format(path, f_name), \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "def symbols_not_accounted_for(seen_count):\n",
    "    for item in sorted(dct.items(), key=lambda x: -x[1]):\n",
    "        if item[1] > seen_count:\n",
    "            print(item[0])\n",
    "\n",
    "#will be used to expand simple new commands when tokens are not recognized.\n",
    "def simple_replace(s):\n",
    "    #works, needs more testing\n",
    "    match = [x for x in re.findall(r\"\\\\newcommand{(.*?)}{(.*?)}\",s)]\n",
    "    commands = {}\n",
    "    if match:\n",
    "        for tpl in match:\n",
    "            commands[tpl[0]] = lambda x: x.replace(tpl[0],tpl[1])\n",
    "    return commands\n",
    "\n",
    "'''\n",
    "for newcmd in re.findall(r'\\\\newcommand.*',data):\n",
    "    dct.update(simple_replace(newcmd))\n",
    "'''\n",
    "\"\"\" \\\\newcommand{\\\\ri}{\\\\right} -> dct['\\\\ri']('hello \\\\ri') -> 'hello \\\\right' \"\"\"\n",
    "\n",
    "\n",
    "symbols = list()\n",
    "dct = defaultdict(int)\n",
    "tokenizer = mwe.MWETokenizer(separator=\"\")\n",
    "symbols_use_count = defaultdict(int)\n",
    "\n",
    "path = \".\"\n",
    "data = [x.replace(\"\\n\", \"\") for x in read_file(path, \"latex_math.txt\").splitlines()]\n",
    "symbols.extend(data)\n",
    "\n",
    "#for now only single expressions\n",
    "path = \".\"\n",
    "data = [x.replace(\"\\n\", \"\") for x in read_file(path, \"user_defined_symbols_expressions.txt\").splitlines()]\n",
    "symbols.extend(data)\n",
    "\n",
    "\n",
    "data = [x.replace(\"\\n\", \"\") for x in read_file(path, \"latex_misc.txt\").splitlines()]\n",
    "symbols.extend(data)\n",
    "\n",
    "data = [x.replace(\"\\n\", \"\") for x in read_file(path, \"english_vocab.txt\").splitlines()]\n",
    "symbols.extend(data)\n",
    "symbols = list(set(symbols))\n",
    "symbols = sorted(symbols, key=lambda x: -len(x))\n",
    "for symbol in symbols:\n",
    "    add_new_token(symbol)\n",
    "\n",
    "file_data = read_file(\".\", argv[1])\n",
    "file_data = clean_display_latex(file_data)\n",
    "def use_package(s):\n",
    "    import re\n",
    "    match = re.match(r\"(?P<cmd>\\\\usepackage)(?P<package>{[a-z]+(,[a-z]+)*})?\", s)\n",
    "    if match:\n",
    "        cmd = match.group(\"cmd\")\n",
    "        packages = match.group(\"package\")\n",
    "        if packages:\n",
    "            packages = [x.strip() for x in re.split(\"{|}|,\", packages) if x.strip()]\n",
    "            if packages:\n",
    "                return {cmd: packages}\n",
    "\n",
    "\n",
    "def math_mode(s):\n",
    "    import re\n",
    "    match = re.findall(r\"[^\\$](\\$\\$.*?\\$\\$)[^\\$]\", s, re.DOTALL)\n",
    "    single_line = re.findall(r\"[^\\$](\\$.*?\\$)[^\\$]\", s)\n",
    "    match.extend(single_line)\n",
    "    return match\n",
    "\n",
    "\n",
    "def balanced(start, s, left_symbol=r\"{\", right_symbol=r\"}\"):\n",
    "    import re\n",
    "    matched = []\n",
    "    # the pattern should include one instance of the left symbol so that balanced=-1\n",
    "    # if left == right then +- counting fails\n",
    "    # begin syntax handled automatically, so there is no need to include left_right symbols\n",
    "    s = re.sub(r'\\\\newcommand.*','',file_data)\n",
    "    if \"\\\\begin\" in start:\n",
    "        match = re.findall(r\"\\\\begin{(.*?)}\", start)\n",
    "        if match:\n",
    "            match = match[0]\n",
    "            # print(match)\n",
    "            left_symbol = f\"\"\"\\\\begin{{{match}}}\"\"\"\n",
    "            right_symbol = f\"\"\"\\\\end{{{match}}}\"\"\"\n",
    "    print(left_symbol)\n",
    "    if left_symbol != right_symbol:\n",
    "        current_offset = 0\n",
    "        balanced = 0\n",
    "        match = re.finditer(r\"{}\".format(start), s)\n",
    "        # print(match)\n",
    "        for m in match:\n",
    "            if m:\n",
    "                # print(m)\n",
    "                start_offset = m.start()\n",
    "                current_offset = m.end() + 1\n",
    "                balanced = -1\n",
    "                while balanced != 0:\n",
    "                    if (\n",
    "                        s[current_offset : current_offset + len(right_symbol)]\n",
    "                        == right_symbol\n",
    "                    ):\n",
    "                        balanced += 1\n",
    "                    if (\n",
    "                        s[current_offset : current_offset + len(left_symbol)]\n",
    "                        == left_symbol\n",
    "                    ):\n",
    "                        print(\"-1\")\n",
    "                        balanced -= 1\n",
    "                    current_offset += 1\n",
    "                matched.append(s[start_offset:current_offset])\n",
    "                start_offset = current_offset\n",
    "    return matched\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8284e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "07819028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "{\n",
      "{\n",
      "\\begin{equation}\n",
      "\\begin{abstract}\n",
      "\\begin{figure}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "data_dict = defaultdict(list)\n",
    "nltk_data = list()\n",
    "titles = balanced(r\"\\\\title{\",file_data)\n",
    "if titles:\n",
    "    for title in titles:\n",
    "        data_dict['title'].append(title)\n",
    "        file_data = re.sub(re.escape(title),'',file_data)\n",
    "        \n",
    "authors = balanced(r\"\\\\author{\",file_data)\n",
    "if authors:\n",
    "    for author in authors:\n",
    "        data_dict['author'].append(author)\n",
    "        file_data = re.sub(re.escape(author),'',file_data)\n",
    "        \n",
    "affiliations = balanced(r\"\\\\affiliation{\",file_data)\n",
    "if affiliations:\n",
    "    for affiliation in affiliations:\n",
    "        data_dict['affiliation'].append(affiliation)\n",
    "        file_data = re.sub(re.escape(affiliation),'',file_data)\n",
    "        \n",
    "equations = balanced(r\"\\\\begin{equation}\",file_data)\n",
    "if equations:\n",
    "    for equation in equations:\n",
    "        data_dict['equation'].append(equation)\n",
    "        for equation in equations:\n",
    "            file_data = re.sub(re.escape(equation),'',file_data,re.MULTILINE)\n",
    "        \n",
    "abstracts = balanced(r\"\\\\begin{abstract}\",file_data)\n",
    "if abstracts:\n",
    "    for abstract in abstracts:\n",
    "        data_dict['abstract'].append(abstract)\n",
    "        nltk_data.append(abstract)\n",
    "        file_data = re.sub(re.escape(abstract),'',file_data)\n",
    "        \n",
    "\n",
    "figures = balanced(r\"\\\\begin{figure}\",file_data)\n",
    "if figures:\n",
    "    for figure in figures:\n",
    "        data_dict['figure'].append(figure)\n",
    "        file_data = re.sub(re.escape(figure),'',file_data)\n",
    "        \n",
    "bibitems = re.findall(r'\\\\bibitem{.*?}.*?\\n\\n',file_data,re.DOTALL)\n",
    "if bibitems:\n",
    "    for bibitem in bibitems:\n",
    "        data_dict['bibitem'].append(bibitem)\n",
    "        file_data = re.sub(re.escape(bibitem),'',file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc6e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8871a5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label=\\'\\\\begin{abstract}\\'\\ncode_gen=f\\'\\'\\'{label}s = balanced(r\"\\\\{label}\",file_data)\\nif {label}s:\\n    for {label} in {label}s:\\n        data_dict[\\'{label}\\'].append({label})\\n        file_data = re.sub(re.escape({label}),\\'\\',file_data)\\'\\'\\'\\n\\nprint(code_gen.replace(\\'\\\\\\',\\'\\\\\\\\\\'))'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"label='\\\\begin{abstract}'\n",
    "code_gen=f'''{label}s = balanced(r\"\\\\{label}\",file_data)\n",
    "if {label}s:\n",
    "    for {label} in {label}s:\n",
    "        data_dict['{label}'].append({label})\n",
    "        file_data = re.sub(re.escape({label}),'',file_data)'''\n",
    "\n",
    "print(code_gen.replace('\\\\','\\\\\\\\'))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "525c9ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['begin{document}', 'begin{thebibliography}']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('begin{.*?}',file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9be24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,line in enumerate(file_data.splitlines()):\n",
    "    #fd = line\n",
    "    if '\\\\usepackage' in line:\n",
    "        resp = use_package(line)\n",
    "        if resp:\n",
    "            for items in resp.values():\n",
    "                for item in items:\n",
    "                    data_dict['usepackage'].append(item)\n",
    "            continue\n",
    "    if '\\\\documentclass\\[' in line:\n",
    "        data_dict['documentclass'].append(line)\n",
    "        continue\n",
    "        \n",
    "    if '%' in line:\n",
    "        resp = False\n",
    "        resp = re.findall('%.*',line)\n",
    "        if resp:\n",
    "            data_dict['comment'].append(line)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            line = re.sub(r'\\\\begin{document}|\\\\raggedbottom|\\\\maketitle|end{abstract}| \\\n",
    "                          \\\\section{.*?}|end{.*?}|\\\\begin{thebibliography}','',line)\n",
    "            line = re.sub(r'\\\\noindent ',' ',line)\n",
    "            if line.strip():\n",
    "                nltk_data.append(line)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "191226fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_data_v1 = '\\n'.join(nltk_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ea4cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents  = nltk.sent_tokenize(nltk_data_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8ea93c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{abstract}\n",
      "Two dimensionless fundamental physical constants, the fine structure constant $\\alpha$ and the proton-to-electron mass ratio $\\frac{m_p}{m_e}$ are attributed a particular importance from the point of view of nuclear synthesis, formation of heavy elements, planets, and life-supporting structures.\n",
      "\n",
      "\n",
      "Here, we show that a combination of these two constants results in a new dimensionless constant which provides the upper bound for the speed of sound in condensed phases, $v_u$.\n",
      "\n",
      "\n",
      "We find that $\\frac{v_u}{c}=\\alpha(\\frac{m_e}{2m_p})^{\\frac{1}{2}}$, where $c$ is the speed of light in vacuum.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18932/1349422816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    print(sent)\n",
    "    sleep(3)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2eab1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted([x for x in nltk.FreqDist(nltk.ngrams(nltk.wordpunct_tokenize(nltk_data_v1),2)).items() if x[1] > 5],key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b3086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec669887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from time import sleep\n",
    "from sys import argv\n",
    "\n",
    "\n",
    "document = nltk_data_v1\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(\"english\")\n",
    "stemmer = Stemmer(\"english\")\n",
    "parser = PlaintextParser(document, tokenizer)\n",
    "\n",
    "\n",
    "stop_words = [x for x in get_stop_words(\"english\") if len(x) > 1]\n",
    "summarizer = LsaSummarizer(stemmer)\n",
    "summarizer.stop_words = stop_words\n",
    "\n",
    "count = 0\n",
    "summy_sents =[]\n",
    "word_count = 0\n",
    "for sent in summarizer(parser.document, len(document.splitlines())/1):\n",
    "    word_count += len(sent.words)\n",
    "    count+=1\n",
    "    #sleep(5)\n",
    "    #print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd3e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "24c9a07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$\\\\hbar\\\\omega_{D}$',\n",
       " '$\\\\hbar$',\n",
       " '$\\\\alpha$',\n",
       " '$\\\\alpha c$',\n",
       " '$\\\\omega_{D}$',\n",
       " '$\\\\rho$']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The goal is to extract all of the symbols, then find the meaning of each symbol in the tex file.\n",
    "[clean_display_latex(x) for x in \n",
    " set(re.findall(r'\\$.*?\\$',nltk_data_v1)) \n",
    " if not re.findall('frac|=',str(x)) \n",
    " and re.findall('\\\\\\\\',str(x))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder,TrigramAssocMeasures\n",
    "\n",
    "#from nltk.collocations.py source\n",
    "def demo(scorer=None, compare_scorer=None, text=nltk_data_v1):\n",
    "\n",
    "    from nltk.metrics import (\n",
    "        BigramAssocMeasures,\n",
    "        spearman_correlation,\n",
    "        ranks_from_scores,\n",
    "    )\n",
    "\n",
    "    if scorer is None:\n",
    "        scorer = BigramAssocMeasures.likelihood_ratio\n",
    "    if compare_scorer is None:\n",
    "        compare_scorer = BigramAssocMeasures.raw_freq\n",
    "\n",
    "    from nltk.corpus import stopwords, webtext\n",
    "\n",
    "    ignored_words = stopwords.words(\"english\")\n",
    "    word_filter = lambda w: len(w) < 3 or w.lower() in ignored_words\n",
    "\n",
    "    if 1 == 1:\n",
    "        words = [word.lower() for word in nltk.word_tokenize(text)]\n",
    "        \n",
    "        cf = BigramCollocationFinder.from_words(words)\n",
    "        cf.apply_freq_filter(3)\n",
    "        cf.apply_word_filter(word_filter)\n",
    "\n",
    "        corr = spearman_correlation(\n",
    "            ranks_from_scores(cf.score_ngrams(scorer)),\n",
    "            ranks_from_scores(cf.score_ngrams(compare_scorer)),\n",
    "        )\n",
    "        #print(file)\n",
    "        print(\"\\t\", [\" \".join(tup) for tup in cf.nbest(scorer, 25)])\n",
    "        print(\"\\t Correlation to %s: %0.4f\" % (compare_scorer.__name__, corr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "508e48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ['\\\\frac m_p m_e', 'fine structure constant', 'using a=a_ bohr', 'provides upper bound', 'upper bound terms', 'upper bound speed', 'arrive upper bound', 'upper bound sets', 'discussed upper bound', 'representing upper bound', 'new upper bound', 'upper bound v_u', 'upper bound v00', 'fundamental physical constants', 'entropy heat capacity']\n",
      "\t Correlation to raw_freq: -0.6716\n"
     ]
    }
   ],
   "source": [
    "#source from nltk collocations.py file\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import (\n",
    "        BigramAssocMeasures,\n",
    "        spearman_correlation,\n",
    "        ranks_from_scores,\n",
    "    )\n",
    "ignored_words = stopwords.words(\"english\")\n",
    "\n",
    "word_filter = lambda w: len(w) < 3 or w.lower() in ignored_words\n",
    "words = [word.lower() for word in nltk.word_tokenize(nltk_data_v1)]\n",
    "words = [x for x in words if not word_filter(x)]\n",
    "words = [x for x in words if x !='\\\\ref']\n",
    "scorer = TrigramAssocMeasures.likelihood_ratio\n",
    "tgcf = TrigramCollocationFinder.from_words(words)\n",
    "compare_scorer = TrigramAssocMeasures.raw_freq\n",
    "corr = spearman_correlation(\n",
    "            ranks_from_scores(tgcf.score_ngrams(scorer)),\n",
    "            ranks_from_scores(tgcf.score_ngrams(compare_scorer)),\n",
    "        )\n",
    "        \n",
    "print(\"\\t\", [\" \".join(tup) for tup in tgcf.nbest(scorer, 15)])\n",
    "print(\"\\t Correlation to %s: %0.4f\" % (compare_scorer.__name__, corr))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
