,proof
0,"\begin{proof}
  For the first equality, note that the inclusion $\supset$ follows
  from \pref{eq:K}. To prove the other inclusion we need to show that
  any $K \in \mathscr{T}_{N=1}(\Gamma)$ can be written in the form
  \pref{eq:K} with $G$ symmetric and positive definite and $B$
  antisymmetric. To see this write $K = \bigl(\begin{smallmatrix}\alpha & \beta \\ 
    \gamma & \delta%
                                 \end{smallmatrix}\bigr)$.  Now note that $\beta: \Gamma_\R^* \rightarrow
  \Gamma_\R$ has to be invertible, because otherwise $\langle v, q K v
  \rangle = 0$ for $v \in 0 \oplus \Gamma_\R^*$ and that contradicts
  the requirement that $q K$ be positive definite.  Knowing this, one
  can easily show that $K^2 = \operatorname{Id}$ is equivalent to $\gamma =
  \beta^{-1} - \beta^{-1} \alpha^2$ and $\delta = -\beta^{-1} \alpha
  \beta$. Substituting that into $K^t q K = q$, one finds that
  $\beta^{-1}\alpha$ is antisymmetric and $\beta$ is symmetric. So if
  we identify $\beta$ with $G^{-1}$ and $\beta^{-1}\alpha$ with $-B$,
  then $K$ will have the form \pref{eq:K}.  The positive definiteness
  of $G$ follows from the positive definiteness of $qK$ using
  \pref{eq:qKdiag}.
  
  The second alternative is just a reformulation of the first one: $E$
  can be defined as $B+G$ and $B$ and $G$ can be obtained from $E$ as
  the symmetric and antisymmetric part.
  
  To show the third equality note that $K^2 = \operatorname{Id}$ implies that $K$
  is diagonalisable with eigenvalues $\pm 1$.  Let $E_{\pm 1}$ be the
  eigenspaces of $K$ for the eigenvalues $\pm 1$.  Then for all $v \in
  E_{+1} \setminus \{ 0 \}$ we have
\[
  0 < \langle v, qK v \rangle = \langle v, q v \rangle.
\]
So $q$ restricted to $E_{+1}$ is positive definite. Similarly, it
follows that $q$ restricted to $E_{-1}$ is negative definite. This
means that $E_{\pm1}$ can each have at most dimension $m$. Because
$E_{+1} \oplus E_{-1} = \Gamma_\R \oplus \Gamma_\R^*$, we can conclude
that $E_{\pm 1}$ indeed have dimension $m$. So $E_{+1}$ is a maximal
positive subspace. Conversely, if $V$ is a maximal positive subspace
for $q$, then $W := V^\perp$ is a maximal negative subspace. So we can
define an endomorphism $K$ of $\Gamma_\R \oplus \Gamma_\R^* = V \oplus
W$ that is the identity on $V$ and minus the identity on $W$. One can
easily check that $K$ defined in this way satisfies the conditions
stated above in the definition of $\mathscr{T}_{N=1}$.

Alternatively, note that if $E$ is a map $\Gamma_\R \to \Gamma_\R^*$
with positive definite symmetric part, then $\phi_E = \bigl(\begin{smallmatrix}\operatorname{Id}
  \\ E%
                                 \end{smallmatrix}\bigr)$ defines a map $\Gamma_\R \to \Gamma_\R \oplus \Gamma_\R^*$.
One can easily check that the image $V = \im \phi_E$ is a positive
subspace and that $V^\perp = \im \phi_{-E^t}$.
\"
1,"\begin{proof}
  To prove the inclusion $\supset$ it suffices to check that $I$ and
  $J$ defined by
\begin{subequations}
\begin{align}
\label{eq:I}
  I &= \begin{twomatrix}
               \omega^{-1} B      &  -\omega^{-1} \\
         \omega + B \omega^{-1} B & -B \omega^{-1}
       \end{twomatrix}, \\
\label{eq:J}
  J &= \begin{twomatrix}
             j       &   0 \\
         B j + j^t B & -j^t
       \end{twomatrix}.
\end{align}
\end{subequations}
satisfy the conditions from \sref{def:MNtwo}. For the other inclusion,
note that the extra condition in~\pref{eq:Mgeom} ensures that we can
write $J=\bigl(\begin{smallmatrix}\alpha & 0 \\ \beta & \gamma%
                                 \end{smallmatrix}\bigr)$. From $J^2 = -\operatorname{Id}$
it follows that $\alpha^2 = \gamma^2 = -\operatorname{Id}$. Using $J^t q J = q$, we
find that $\gamma = -\alpha^t$. Because of the map $\pi: \TMp_{N=2}
\rightarrow \mathscr{T}_{N=1}$ we know that $K := IJ$ can be written in the
form \pref{eq:K}. So we can express $I = -KJ$ in terms of $\alpha$,
$\beta$, $G$ and $B$. From $I^2 = -\operatorname{Id}$ it follows that $\beta = B
\alpha + \alpha^t B$ (block in the upper right hand corner). If we now
define $\omega := - G \alpha$ and $j := \alpha$, then using again $I^2
= -\operatorname{Id}$ (block in the lower right hand corner) it follows that $j^t
\omega j = \omega$. This shows that $G = \omega j$, so $\omega j$ is
positive definite. Finally the antisymmetry of $\omega$ follows from
$I^t q I = q$.
\"
2,"\begin{proof}
  The alternative description of $\TMp_{N=2}$ follows from the
  discussion above, so we only need to prove the last statement.  If
  we write $J: \Gamma_\R \oplus \Gamma_\R^* \rightarrow \Gamma_\R
  \oplus \Gamma_\R^*$ in block form, the condition defining
  $\TMp_{N=2}^{\mathrm{geom}}$ corresponds to the vanishing of the
  block in the upper left which is a map from $\Gamma_\R^*$ to
  $\Gamma_\R$.  Using \pref{eq:Jdiag} one can easily check that this
  map is given by $\frac{1}{2}(j_1 - j_2)G^{-1}$. So we see that the
  geometrical case is precisely when $j_1 = j_2 = j$. In hindsight
  this justifies the use of the notation $\TMp_{N=2}^{\mathrm{geom}}$
  for the space defined in \pref{eq:Mgeom}.
\"
3,"\begin{proof}
  Let $(I,J)$ correspond to $(B,\omega,j) \in
  \TMp_{N=2}^{\mathrm{geom}}(\Gamma)$ according to \pref{eq:I} and
  \pref{eq:J}. Using $\epsilon_L \epsilon_R = -1$ and the expressions
  for $I'$ and $J'$ from \sref{tab:genmor}, we find $(I',J') =
  \mu_{g_{\Gamma_1,\Gamma_2}}^{(\epsilon_L, \epsilon_R)}(I,J) =
  \epsilon_R (g J g^{-1}, g I g^{-1})$. The condition that $(I',J')$
  be in $\TMp_{N=2}^{\mathrm{geom}}(\Gamma')$ is equivalent to $J'$
  satisfying $J'(0 \oplus {\Gamma'}^*) \subset 0 \oplus {\Gamma'}^*$.
  Because $\epsilon_R J'$ is given by \pref{eq:Jp}, this condition is
  equivalent to
\[
  \begin{twomatrix}
    I_{x\check{x}}     &    I_{xy} \\
    I_{\check{y}\check{x}} & I_{\check{y}y}
  \end{twomatrix} = 0.
\]
From $I_{x\check{x}} = 0$ it follows using \pref{eq:I} that
$\omega_{\check{y}y} = 0$. Here it is essential that $\Gamma_1$ and
$\Gamma_2$ have equal rank.  Combining this with $I_{xy} = 0$ we find
$B_{\check{y}y} = 0$. It can easily be checked that these two
conditions also guarantee that the remaining components vanish.
\"
4,"\begin{proof}
  Given a gluing matrix $R$ satisfying the
  conditions~\ref{cond:ortho}, \ref{cond:rat1}, and~\ref{cond:rat2},
  we can take $L$ to be the sublattice $L_{R,K} \subset \Gamma \oplus
  \Gamma^*$ defined by \pref{eq:Ldef}. Using $\Gamma_\R = V \oplus W$
  we can rewrite this equation as two equations, one for the
  $V$-component and one for the $W$-component. These two equations can
  be simplified further using \pref{eq:stdR} and $\tilde{R} = (\tilde{G}
  - \mathcal{F})^{-1}(\tilde{G} + \mathcal{F})$. After some rewriting
  we find
  \begin{align}
  \label{eq:LRV}
    w_V &= 0, \\
    (m - B w)_{W^*} &= \mathcal{F} w_W. \notag
  \end{align}
  Here the subscripts $V$, $W$, $V^*$, and $W^*$ label the components
  and we also used $\Gamma^*_\R = V^* \oplus W^*$.  The first equation
  implies that $w \in W$, so we can drop that equation and use the
  second equation to describe $L$ as a sublattice of $(W \cap \Gamma)
  \oplus \Gamma^*$. The fact that $w \in W$ allows us to omit the
  subscript $W$ and to replace $(B w)_{W^*}$ by $\tilde{B} w$. If we
  also use $\mathcal{F} = F - \tilde{B}$, we are left with the
  following equation defining $L$ as a sublattice of $(W \cap \Gamma)
  \oplus \Gamma^*$
  \begin{equation}
  \label{eq:LRW}
    m_{W^*} = F w.
  \end{equation}
  This vector equation stands for $k$ independent equations. More
  explicitly, we can choose an integral basis $\{ e_i \}$ for $W \cap
  \Gamma$. Then for every $i=1,\dots,k$ we obtain an equation
  \[
    \langle e_i, m_{W^*} \rangle = \langle e_i, m \rangle
    = \langle e_i, F w \rangle.
  \]
  Because of the second rationality condition~\ref{cond:rat2}, these
  equations are linear with rational coefficients. So the lattice $L$
  is a codimension $k$ sublattice in the $(k+\rk \Gamma)$-dimensional
  lattice $(W \cap \Gamma) \oplus \Gamma^*$. Therefore the rank of the
  lattice $L_{R,K}$ is indeed equal to the rank of $\Gamma$.
  
  To see that the restriction of $q$ to $L$ vanishes we compute
  \[
  \begin{split}
    q \bigl (\bigl(\begin{smallmatrix}w^{\vphantom{\prime%
                                 \end{smallmatrix}\bigr)}\\m^{\vphantom{\prime}}}, 
             \bigl(\begin{smallmatrix}w'\\m'%
                                 \end{smallmatrix}\bigr) \bigr ) 
    &= \langle w, m' \rangle + \langle w', m \rangle 
     = \langle w_V^{\vphantom{\prime}}, m_{V^*}' \rangle 
       + \langle w_W^{\vphantom{\prime}}, m_{W^*}' \rangle 
       + \langle w_V', m_{V^*}^{\vphantom{\prime}} \rangle
       + \langle w_W', m_{W^*}^{\vphantom{\prime}} \rangle\\
    &= \langle w_W^{\vphantom{\prime}}, F w_W' \rangle 
       + \langle w_W', F w_W^{\vphantom{\prime}} \rangle = 0.
  \end{split}
  \]
  Here we used the antisymmetry of $F$ in the final step. This shows
  that we can associate a sublattice $L$ satisfying the conditions from
  the statement of the theorem to any gluing matrix $R$ satisfying the
  conditions~\ref{cond:ortho}, \ref{cond:rat1}, and~\ref{cond:rat2}.
  
  Conversely, if we start with such a sublattice $L$, then we find a
  sublattice of $\Gamma$ by projecting $L \subset \Gamma \oplus
  \Gamma^*$ to $\Gamma$. Tensoring this sublattice with $\R$ yields a
  subspace $W$ satisfying $\rk (W \cap \Gamma) = \dim_\R W$.  Using
  $G$ we can define $V$ as the orthogonal complement of $W$, and $V^*$
  and $W^*$ as $G(V)$ and $G(W)$ respectively. As above, we can express
  $q$ in terms of the components. Let $(w,m)$ and $(w',m')$ be
  elements of the lattice $L$. Because $w_V^{\vphantom{\prime}}$ and
  $w_V'$ vanish, we find
  \[
    q \bigl (\bigl(\begin{smallmatrix}w^{\vphantom{\prime%
                                 \end{smallmatrix}\bigr)}\\m^{\vphantom{\prime}}}, 
             \bigl(\begin{smallmatrix}w'\\m'%
                                 \end{smallmatrix}\bigr) \bigr ) 
    = \langle w_W^{\vphantom{\prime}}, m_{W^*}' \rangle 
      + \langle w_W', m_{W^*}^{\vphantom{\prime}} \rangle.
  \]
  As $q|_L$ vanishes, this implies $\langle w_W',
  m_{W^*}^{\vphantom{\prime}} \rangle = -\langle
  w_W^{\vphantom{\prime}}, m_{W^*}' \rangle$. We can choose a basis
  $e_i \in W \cap \Gamma$ of $W$, such that there exist $f_i \in
  \Gamma^*$ satisfying $(e_i,f_i) \in L$. Substituting $(e_i,f_i)$ for
  $(w',m')$, we find $\langle e_i, m_{W^*}^{\vphantom{\prime}} \rangle
  = -\langle w_W^{\vphantom{\prime}}, f_{i,W^*} \rangle$. It follows
  that $m_{W^*}$ is determined by $w_W$. Because $L$ is a sublattice,
  we must have $m_{W^*} = F w_W$ for some linear map $F: W \rightarrow
  W^*$. The map $F$ has to be antisymmetric for $q|_L$ to vanish. The
  fact that $L$ is a lattice of rank $\rk \Gamma$ ensures that $F$ is
  rational.  Using $F$ we can define $\mathcal{F}$ as $F-\tilde{B}$
  and $\tilde{R}$ as $(\tilde{G} - \mathcal{F})^{-1}(\tilde{G} +
  \mathcal{F})$. Finally, the gluing matrix $R$ can be defined as the
  block matrix \pref{eq:stdR} with respect to $\Gamma_\R = V \oplus
  W$.
\"
5,"\begin{proof}
  The condition \pref{eq:Ldef} defining $L_{R,K}$ can be written as
  \[
    \begin{twomatrix} G^{-1} & 0 \\ 0 & -R G^{-1} \end{twomatrix}
    \begin{twomatrix} E^t & \operatorname{Id} \\ -E & \operatorname{Id} \end{twomatrix}
    \begin{twomatrix} w \\ m \end{twomatrix}
    = \begin{twomatrix} \operatorname{Id} & 0 \\ 0 & R \end{twomatrix}
      \mathcal{R}(G,B) \begin{twomatrix} w \\ m \end{twomatrix}
    \in \Delta,
  \]
  where $\Delta$ is the diagonal in $\Gamma_\R \oplus
  \Gamma_\R^*$. In other words the lattice $L_{R,K}$ can be defined as
  \[
    L_{R,K} = \mathcal{R}(G,B)^{-1} \begin{twomatrix} \operatorname{Id} & 0 \\ 
          0 & R^{-1} \end{twomatrix} \Delta.
  \]
  Applying the lattice isomorphism $g$ to both sides, we obtain
  \[
  \begin{split}
    g(L_{R,K}) &= g \mathcal{R}(G,B)^{-1} \begin{twomatrix} \operatorname{Id} & 0 \\ 
          0 & R^{-1} \end{twomatrix} \Delta
     = \mathcal{R}(G',B')^{-1} \begin{twomatrix} (a+bE) & 0 \\ 
          0 & (a-bE^t) R^{-1} \end{twomatrix} \Delta \\
    &= \mathcal{R}(G',B')^{-1} \begin{twomatrix} \operatorname{Id} & 0 \\ 
          0 & {R'}^{-1} \end{twomatrix} \Delta
     = L_{R',K'}.
  \end{split}
  \]
  The second equality is based on \pref{eq:gR}. For the next step we
  used the definition of $R'$ and the fact that $\diag(a+bE,
  a+bE)$ leaves $\Delta$ invariant.
\"
6,"\begin{proof}
  As already observed above, \sref{thm:LR} and \sref{prop:transLR}
  together imply that $R'$ satisfies the conditions~\ref{cond:ortho},
  \ref{cond:rat1}, and~\ref{cond:rat2} in terms of the metric and
  B-field corresponding to $K'$. To see that $\Xi' = g \Xi \in
  \mathrm{CP}(K', R', r)$, note that the lattice isomorphism $g$ only
  acts on the $\Gamma_\R \oplus \Gamma_\R^*$ part of the tensor
  product $(\Gamma_\R \oplus \Gamma_\R^*) \otimes M_r(\C)$. Therefore
  the components $\Xi'_i \in M_r(\C)$ of $\Xi'$ are linear
  combinations with real (even integral) coefficients of the
  components of $\Xi$.  This shows that they again commute and have
  real eigenvalues.  Because $g$ is a lattice isomorphism $(\Gamma
  \oplus \Gamma^*,q) \rightarrow (\Gamma' \oplus {\Gamma'}^*, q')$ and
  $L_{R',K'} = g(L_{R,K})$, mapping $\Xi$ to $g \Xi$ is compatible with the
  equivalence relation $\sim$ and $\Xi'$ is a well defined element of
  $\mathrm{CP}(K', R', r)$. Combining all this it follows that
  $(R',\Xi') \in C_{K'}^r(X')$. Bijectivity follows from the
  functoriality property $\phi_{g_1} \circ \phi_{g_2} = \phi_{g_1
    \circ g_2}$, which is easily verified.

To see that $\phi_g$ is compatible with the map $f_g$ on the boundary
states, we first compute $f_g$ on Ishibashi states
\[
\begin{split}
  f_g \lvert R,w,m \rangle\!\rangle &= \exp \bigl (-\sum_{\ell=1}^\infty
  \frac{1}{\ell} G(f_g \alpha_{-\ell} f_g^{-1}, R f_g
  \bar{\alpha}_{-\ell} f_g^{-1} ) \bigr )
  f_g \lvert w,m \rangle \\
  &= \exp \bigl (-\sum_{\ell=1}^\infty
  \frac{1}{\ell} G((a+bE)^{-1} \alpha'_{-\ell}, R (a-bE^t)^{-1}
  \bar{\alpha}'_{-\ell}) \bigr ) \lvert g(w,m) \rangle\\
  &= \lvert R',g(w,m) \rangle\!\rangle.
\end{split}
\]
Here we started with the definition of the Ishibashi states in
\pref{eq:Ishi} and then used the transformation of the
$\alpha_{-\ell}$'s and $\bar{\alpha}_{-\ell}$'s from \pref{eq:fggens}
and finally used the expressions for $G'$ and $R'$ from
\pref{tab:trules}. Combining this with the definition
\pref{eq:bdystateXi} of boundary states, we can easily verify the
required compatibility.
\"
7,"\begin{proof}
  We use the alternative description \pref{eq:Xiequiv} of the
  equivalence relation, i.e., we write $\Xi' = \Xi + Y + \xi \operatorname{Id}_r$,
  where $Y \in L_{R,K} \otimes M_r(\C)$ and $\xi \in \Gamma \oplus
  \Gamma^*$. Let us define a map $\phi: M \rightarrow M'$ by
\[
  \phi(s(x)) = (e^{\langle \frac{\partial}{\partial x}, \Xi'_W - \xi_W
  \rangle}) (e^{-\langle \frac{\partial}{\partial x}, \Xi_W
  \rangle}) s(x)
\]
We should check that $\phi(s)$ is indeed in $M'$ and that the map
$\phi$ is compatible with all the structures present. Because $s$ is
in $M$, we know that
\[
  s(x+\lambda) = e(x,\lambda) s(x) = e^{2 \pi i\langle F(x + \Xi_W),
  \lambda \rangle} s(x).
\]
For our computations the following identity will be very useful
\begin{equation}
\label{eq:transcomm}
  e^{\langle \frac{\partial}{\partial x}, A\rangle}(g(x)h(x)) =
  (e^{\langle \frac{\partial}{\partial x}, A\rangle} g(x))
  (e^{\langle \frac{\partial}{\partial x}, A\rangle} h(x)).
\end{equation}
This holds whenever all components $A_i$ of $A$ commute with $g(x)$.
When applying $\phi$ to the expression for $s(x+\lambda)$, we use this
twice to find
\[
  \phi(s(x+\lambda)) = (e^{\langle \frac{\partial}{\partial x},
    \Xi'_W -\xi_W \rangle}) \bigl (e^{2 \pi i\langle Fx, \lambda \rangle} 
  (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle} s(x))\bigr )
  = e^{2 \pi i\langle F(x+\Xi'_W), \lambda \rangle} \phi(s(x)).
\]
In the first step we could apply \pref{eq:transcomm}, because all
components of $\Xi$ commute. That turns the first factor into a
scalar, so that also in the second step there are no problems with non
commuting matrices. We omitted $\xi_W$ in the final expression,
because $\langle F \xi_W, \lambda \rangle$ is integral because of our
assumption that $F$ is integral. This shows that $\phi(s)$ is in $M'$.
The computation for the connection is slightly more complex
\[
\begin{split}
  \phi(\nabla s(x)) &= \phi(\dop s(x) + 2\pi i \langle F x + \Xi_{W^*},
  \dop x \rangle s(x)) \\
  &= (e^{\langle \frac{\partial}{\partial x}, \Xi'_W - \xi_W \rangle})
  (\dop (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle} s(x))\\
  & \qquad + 2\pi i \langle Fx-F\Xi_W  + \Xi_{W^*}, \dop x \rangle 
    (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle} s(x)))\\
  &= (e^{\langle \frac{\partial}{\partial x}, \Xi'_W - \xi_W \rangle})
  (\dop (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle} s(x))\\ 
  &\qquad + 2\pi i \langle Fx-F\Xi'_W+\Xi'_{W^*}+F\xi_W -\xi_{W^*}, 
    \dop x \rangle 
    (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle} s(x)))\\
  &= \dop (\phi(s(x))) + 2\pi i \langle Fx+\Xi'_{W^*} - \xi_{W^*},
    \dop x \rangle \phi(s(x)).
\end{split}
\]
Here we used again $\Xi' = \Xi + Y + \xi \operatorname{Id}_r$. Because $Y$ is in
$L_{R,K} \otimes M_r(\C)$, we know that $FY_W = Y_{W^*}$ (see
\pref{eq:LRW}).  Now recall that $\xi$ is an element of $\Gamma \oplus
\Gamma^*$. This means that $\langle \xi_{W^*}, \dop x \rangle$ defines
an integral 1-form on $W/(\Gamma \cap W)$, so $\phi$ takes the
connection on $M$ to a connection on $M'$ equivalent to $\nabla'$.
Finally we check the compatibility of $\phi$ with the module structure
\[
\begin{split}
  \phi(f \cdot s(x)) &= (e^{\langle \frac{\partial}{\partial x},
    \Xi'_W - \xi_W \rangle}) (e^{-\langle \frac{\partial}{\partial x}, 
    \Xi_W \rangle}) ((e^{\langle \frac{\partial}{\partial x}, 
    \Xi_X \rangle}f)(x) s(x)) \\
  &= (e^{\langle \frac{\partial}{\partial x}, \Xi'_W - \xi_W \rangle})
    ((e^{\langle \frac{\partial}{\partial x}, \Xi_V \rangle}f)(x) 
    (e^{-\langle \frac{\partial}{\partial x}, \Xi_W \rangle}) s(x))\\
  &= (e^{\langle \frac{\partial}{\partial x}, \Xi'_X - \xi_X \rangle}
     f)(x) \phi(s(x)) = f \cdot \phi(s(x)).
\end{split}
\]
Because $Y_V$ vanishes (see \pref{eq:LRV}), we have $\Xi_V = \Xi'_V -
\xi_V$. In the last step we use that $f$ is periodic, so $f(x-\xi_X) =
f(x)$.
\"
